{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../loaders/imports.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import gen_covariance, gen_beta2, gen_data, get_cov_list\n",
    "from utils import selection_accuracy\n",
    "from sklearn.linear_model import LassoLars, lasso_path, LinearRegression\n",
    "\n",
    "from pyuoi.linear_model import UoI_Lasso\n",
    "from pyuoi.linear_model.adaptive import (mBIC, eBIC, mBIC2, BIC, MIC, \n",
    "bayesian_log_ll, bayesian_lambda_selection, L1_bayes)\n",
    "from sklearn.linear_model import lars_path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity estimation on the basis of BIC\n",
    "def sparsity_estimator0(X, y, n_boots = 48, train_frac = 0.75):\n",
    "    \n",
    "    sparsity_estimates = np.zeros(n_boots)\n",
    "    \n",
    "    n_features, n_samples = X.shape\n",
    "    \n",
    "    for boot in range(n_boots):\n",
    "        # Draw bootstraps\n",
    "        idxs_train, idxs_test = train_test_split(np.arange(X.shape[0]), \n",
    "                                                 train_size = train_frac,\n",
    "                                                 test_size = 1 - train_frac)\n",
    "        Xb = X[idxs_train]\n",
    "        yb = y[idxs_train]\n",
    "\n",
    "        Xb = StandardScaler().fit_transform(Xb)\n",
    "        yb -= np.mean(yb)\n",
    "\n",
    "        alphas, _, coefs  = lars_path(Xb, yb.ravel(), method = 'lasso')\n",
    "\n",
    "        y_pred = Xb @ coefs\n",
    "\n",
    "        # Assess BIC on the LARS path to estimate the sparsity. \n",
    "        BIC_scores = np.array([MIC(yb.ravel(), y_pred[:, j].ravel(),\n",
    "                                   np.count_nonzero(coefs[:, j]), np.log(n_samples)) \n",
    "                               for j in range(coefs.shape[1])])  \n",
    "        \n",
    "        sparsity_estimates[boot] = float(np.count_nonzero(\n",
    "                            coefs[:, np.argmin(BIC_scores)]))/float(n_features)\n",
    "\n",
    "    return sparsity_estimates\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine sparsity estimate using mBIC\n",
    "def sparsity_estimator1(X, y, s0, n_boots = 48, train_frac = 0.75):\n",
    "        \n",
    "    sparsity_estimates = np.zeros(n_boots)\n",
    "    \n",
    "    for boot in range(n_boots):\n",
    "        # Draw bootstraps\n",
    "        idxs_train, idxs_test = train_test_split(np.arange(X.shape[0]), \n",
    "                                                 train_size = train_frac,\n",
    "                                                 test_size = 1 - train_frac)\n",
    "        Xb = X[idxs_train]\n",
    "        yb = y[idxs_train]\n",
    "\n",
    "        Xb = StandardScaler().fit_transform(Xb)\n",
    "        yb -= np.mean(yb)\n",
    "\n",
    "        alphas, _, coefs  = lars_path(Xb, yb.ravel(), method = 'lasso')\n",
    "\n",
    "        y_pred = Xb @ coefs\n",
    "        \n",
    "        mBIC_scores = np.zeros(coefs.shape[1])\n",
    "        \n",
    "        for j in range(coefs.shape[1]): \n",
    "        \n",
    "            ll_, p1_, BIC_, BIC2_, BIC3_, M_k_, P_M_ = bayesian_lambda_selection(yb, y_pred[:, j], n_features, \n",
    "                                                                                 np.count_nonzero(coefs[:, j]),\n",
    "                                                                                 s0, 0)\n",
    "            mBIC_scores[j] = 2 * ll_ - BIC_ - BIC2_ + BIC3_ + P_M_ \n",
    "            \n",
    "        sparsity_estimates[boot] = float(np.count_nonzero(\n",
    "                            coefs[:, np.argmax(mBIC_scores)]))/float(n_features)\n",
    "\n",
    "    return sparsity_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using adaptive BIC criteria given sparsity estimates\n",
    "def adaptive_BIC_estimator(X, y, sparsity_estimates, n_boots = 48, train_frac = 0.75):\n",
    "        \n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    np1 = 100\n",
    "    penalties = np.linspace(0, 5 * np.log(n_samples), np1)\n",
    "    \n",
    "    oracle_penalty = np.zeros(n_boots)\n",
    "    bayesian_penalty = np.zeros(n_boots)\n",
    "\n",
    "    estimates = np.zeros((n_boots, n_features))\n",
    "    oracle_estimates = np.zeros((n_boots, n_features))\n",
    "\n",
    "    \n",
    "    for boot in range(n_boots):\n",
    "        # Draw bootstraps\n",
    "        idxs_train, idxs_test = train_test_split(np.arange(X.shape[0]), \n",
    "                                                 train_size = train_frac,\n",
    "                                                 test_size = 1 - train_frac)\n",
    "        Xb = X[idxs_train]\n",
    "        yb = y[idxs_train]\n",
    "\n",
    "        Xb = StandardScaler().fit_transform(Xb)\n",
    "        yb -= np.mean(yb)\n",
    "\n",
    "        alphas, _, coefs  = lars_path(Xb, yb.ravel(), method = 'lasso')\n",
    "\n",
    "        supports = (coefs.T != 0).astype(bool)\n",
    "\n",
    "        # Stick the true model in there\n",
    "        # supports = np.vstack([supports, (beta.ravel() !=0).astype(bool)])\n",
    "\n",
    "        sa = selection_accuracy(beta.ravel(), supports)\n",
    "        # Keep track of oracle performance\n",
    "        MIC_scores_ = np.zeros((supports.shape[0], np1))\n",
    "\n",
    "        boot_estimates = np.zeros((supports.shape[0], n_features))\n",
    "\n",
    "        models = []\n",
    "        \n",
    "        for j in range(supports.shape[0]):\n",
    "            \n",
    "            support = supports[j, :]\n",
    "\n",
    "            if np.count_nonzero(1 * support > 0):\n",
    "                model = LinearRegression().fit(X[:, support] , y)\n",
    "                boot_estimates[j, support] = model.coef_.ravel()\n",
    "                y_pred = model.predict(X[:, support])\n",
    "                models.append(model)\n",
    "            else:\n",
    "                y_pred = np.zeros(y.size)\n",
    "                models.append(np.nan)\n",
    "                \n",
    "            MIC_scores_[j, :] =  np.array([MIC(y.ravel(), y_pred.ravel(), \n",
    "                                           np.count_nonzero(1 * support), penalty) \n",
    "                                           for penalty in penalties])\n",
    "\n",
    "        selected_models = np.argmin(MIC_scores_, axis = 0)\n",
    "        \n",
    "        bayes_factors = np.zeros(np1)\n",
    "\n",
    "        for i3, penalty in enumerate(penalties):\n",
    "\n",
    "            support = supports[selected_models[i3], :]\n",
    "            yy = models[selected_models[i3]].predict(X[:, support])\n",
    "\n",
    "            ll_, p1_, BIC_, BIC2_, BIC3_, M_k_, P_M_ = bayesian_lambda_selection(\n",
    "                                                       y, yy, n_features, \n",
    "                                                       np.count_nonzero(1 * support),\n",
    "                                                       sparsity_estimates, penalty)\n",
    "\n",
    "            # Add things up appropriately\n",
    "            bayes_factors[i3] = 2 * ll_ - BIC_ - BIC2_ + BIC3_ - p1_ + P_M_\n",
    "\n",
    "\n",
    "        # Select the penalty based on the highest bayes factors \n",
    "        bidx = np.argmax(bayes_factors)\n",
    "\n",
    "        # Save the penalty strength and the chosen model\n",
    "        bayesian_penalty[boot] = penalties[bidx]\n",
    "\n",
    "        # For MIC scores, record the oracle selection accuracy and the oracle penalty\n",
    "        MIC_selection_accuracies = [selection_accuracy(beta.ravel(), \n",
    "                                    supports[selected_models[j], :]) \n",
    "                                    for j in range(selected_models.size)]\n",
    "\n",
    "        # For MIC scores, record the oracle selection accuracy and the orcale penalty \n",
    "\n",
    "        oracle_penalty[boot] = penalties[np.argmax(MIC_selection_accuracies)]\n",
    "#        MIC_oracle_sa_[boot] = np.max(MIC_selection_accuracies)    \n",
    "#        bMIC_sa_[boot] = MIC_selection_accuracies[bidx]\n",
    "        \n",
    "        # Record the best estimate on this bootstrap as determined by the oracle\n",
    "        # and by our procedure\n",
    "        estimates[boot, :] = boot_estimates[selected_models[bidx], :]\n",
    "        oracle_estimates[boot, :] = \\\n",
    "        boot_estimates[selected_models[np.argmax(MIC_selection_accuracies)], :]\n",
    "\n",
    "    # Take the median and record final selection accuracies\n",
    "    final_bayes_estimates = np.median(estimates, axis = 0)\n",
    "    final_oracle_estimates = np.median(oracle_estimates, axis = 0)\n",
    "\n",
    "    return final_bayes_estimates, final_oracle_estimates, oracle_penalty, bayesian_penalty, estimates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-7-c33f474c835a>(47)<module>()\n",
      "-> sparsity_estimates_ = np.count_nonzero(estimates, axis = 1).astype(float)/n_features\n",
      "(Pdb) quit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c33f474c835a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Obtain sparsity estimates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msparsity_estimates_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0msparsity_estimates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparsity_estimates_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c33f474c835a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Obtain sparsity estimates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msparsity_estimates_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0msparsity_estimates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparsity_estimates_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nse/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nse/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_features = 50\n",
    "ns = 200\n",
    "sparsity = np.linspace(0.05, 1, 11)\n",
    "\n",
    "# Covariance of design matrix\n",
    "sigma = gen_covariance(n_features, 0, n_features, 5, 1)\n",
    "\n",
    "n_boots = 48\n",
    "train_frac = 0.75\n",
    "\n",
    "# Iteratively feed the sparsity estimate back into the algorithm\n",
    "n_iter = 5\n",
    "\n",
    "# Things to be recorded:\n",
    "sparsity_estimates = np.zeros((sparsity.size, n_iter + 2,  n_boots))\n",
    "# bayesian_penalty = np.zeros((sparsity.size, n_boots))\n",
    "# oracle_penalty = np.zeros((sparsity.size, n_boots))\n",
    "# MIC_oracle_sa_ = np.zeros((sparsity.size, n_boots))\n",
    "# bMIC_sa_ = np.zeros((sparsity.size, n_boots))\n",
    "# MIC_oracle_sa = np.zeros(sparsity.size)\n",
    "# bMIC_sa = np.zeros(sparsity.size)\n",
    "\n",
    "final_bayes_estimates = np.zeros((sparsity.size, n_features))\n",
    "final_oracle_estimates = np.zeros((sparsity.size, n_features))\n",
    "\n",
    "for i, s in enumerate(sparsity):\n",
    "    t0 = time.time()\n",
    "    beta = gen_beta2(n_features, n_features, sparsity = s, betawidth = 0)\n",
    "    X, X_test, y, y_test, ss = gen_data(ns, n_features, kappa = 5, \n",
    "                                        covariance = sigma, beta = beta)\n",
    "\n",
    "    # Initial sparsity estimates\n",
    "    sparsity_estimates_ = sparsity_estimator0(X, y)\n",
    "    sparsity_estimates[i, 0, :] = sparsity_estimates_\n",
    "    # Refine \n",
    "    sparsity_estimates_ = sparsity_estimator1(X, y, sparsity_estimates_)\n",
    "    sparsity_estimates[i, 1, :] = sparsity_estimates_\n",
    "\n",
    "    \n",
    "    for j in range(n_iter):\n",
    "    \n",
    "        # Fit\n",
    "        fbe, foe, op, bp, estimates = adaptive_BIC_estimator(X, y, sparsity_estimates_)\n",
    "        pdb.set_trace()\n",
    "\n",
    "        # Obtain sparsity estimates \n",
    "        sparsity_estimates_ = np.count_nonzero(estimates, axis = 1).astype(float)/n_features\n",
    "        sparsity_estimates[i, j + 2, :] = sparsity_estimates_\n",
    "    \n",
    "        \n",
    "    \n",
    "    print(time.time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
