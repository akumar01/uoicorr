{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../loaders/imports.py\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import gen_covariance, gen_beta2, gen_data, get_cov_list\n",
    "from utils import selection_accuracy\n",
    "from sklearn.linear_model import LassoLars, lasso_path, LinearRegression\n",
    "\n",
    "from pyuoi.linear_model import UoI_Lasso\n",
    "from pyuoi.linear_model.adaptive import mBIC, eBIC, mBIC2, BIC, MIC, bayesian_log_ll, bayesian_lambda_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> c:\\users\\akumar\\nse\\pyuoi\\pyuoi\\linear_model\\adaptive.py(37)bayesian_lambda_selection()\n",
      "-> return 0\n",
      "(Pdb) ll\n",
      "  7  \tdef bayesian_lambda_selection(y, y_pred, n_features, model_size, sparsity_prior, penalty):\n",
      "  8  \t\n",
      "  9  \t    y = y.ravel()\n",
      " 10  \t    y_pred = y_pred.ravel()\n",
      " 11  \t\n",
      " 12  \t    n_samples = y.size\n",
      " 13  \t\n",
      " 14  \t    # Log likelihood\n",
      " 15  \t    ll = log_likelihood_glm('normal', y, y_pred)\n",
      " 16  \t\n",
      " 17  \t    # Regularization Penalty\n",
      " 18  \t    p1 = 2 * penalty * model_size\n",
      " 19  \t\n",
      " 20  \t    # Normal BIC penalty\n",
      " 21  \t    BIC = model_size * np.log(n_samples)\n",
      " 22  \t\n",
      " 23  \t    # Second order Bayes factor approximation\n",
      " 24  \t    RSS = np.sum((y - y_pred)**2)\n",
      " 25  \t    BIC2 = n_samples**3/(2 * RSS*2)\n",
      " 26  \t\n",
      " 27  \t    # Term arising from normalization\n",
      " 28  \t    BIC3 = model_size * np.log(2 * np.pi)\n",
      " 29  \t\n",
      " 30  \t    # Model probability prior\n",
      " 31  \t    M_k = scipy.special.binom(n_features, model_size) * \\\n",
      " 32  \t          sparsity_prior**model_size * (1 - sparsity_prior)**(n_features - model_size)\n",
      " 33  \t    P_M = 2 * np.log(M_k)\n",
      " 34  \t\n",
      " 35  \t    pdb.set_trace()\n",
      " 36  \t\n",
      " 37  ->\t    return 0\n",
      "(Pdb) !ll\n",
      "-487.3666281128185\n",
      "(Pdb) p1\n",
      "0\n",
      "(Pdb) BIC\n",
      "0.0\n",
      "(Pdb) BIC2\n",
      "207.82036792973693\n",
      "(Pdb) BIC3\n",
      "0.0\n",
      "(Pdb) M_k\n",
      "0.07694497527671315\n",
      "(Pdb) P_M\n",
      "-5.129329438755058\n",
      "(Pdb) sparsity_prior\n",
      "0.05\n",
      "(Pdb) np.log(sparsity_prior**model_size * (1 - sparsity_prior)**(n_features - model_size))\n",
      "-2.564664719377529\n",
      "(Pdb) quit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-33f2d25ed5ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mbll_scores_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbayesian_lambda_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msupport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         MIC_scores_[j, :] =  np.array([MIC(y, y_pred, np.count_nonzero(1 * support), penalty) \n",
      "\u001b[1;32mc:\\users\\akumar\\nse\\pyuoi\\pyuoi\\linear_model\\adaptive.py\u001b[0m in \u001b[0;36mbayesian_lambda_selection\u001b[1;34m(y, y_pred, n_features, model_size, sparsity_prior, penalty)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBIC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRSS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBIC2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBIC3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_M\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;31m# Bayes factor approximation with a prior probability on models placed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;31m# according to their sparsity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbayesian_log_ll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparsity_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\akumar\\nse\\pyuoi\\pyuoi\\linear_model\\adaptive.py\u001b[0m in \u001b[0;36mbayesian_lambda_selection\u001b[1;34m(y, y_pred, n_features, model_size, sparsity_prior, penalty)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBIC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRSS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBIC2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBIC3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_M\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;31m# Bayes factor approximation with a prior probability on models placed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;31m# according to their sparsity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbayesian_log_ll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparsity_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nse\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nse\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load ../loaders/datgen.py\n",
    "n_features = 50\n",
    "n_samples = 200\n",
    "\n",
    "\n",
    "# Iterate over sparsity. Yield a sequences of models using LARS. Choose a model using the various extensions \n",
    "# of the BIC. \n",
    "\n",
    "sparsity = np.linspace(0.05, 1, 11)\n",
    "sigma = gen_covariance(n_features, 0, n_features, 5, 1)\n",
    "\n",
    "oracle_penalty = np.zeros(sparsity.size)\n",
    "MIC_oracle_sa = np.zeros(sparsity.size)\n",
    "\n",
    "bll_scores = []\n",
    "BIC_scores = []\n",
    "mBIC_scores = []\n",
    "eBIC_scores = []\n",
    "mBIC2_scores = []\n",
    "\n",
    "# Oracle sa\n",
    "sa = []\n",
    "\n",
    "\n",
    "for i, s in enumerate(sparsity):\n",
    "\n",
    "    beta = gen_beta2(n_features, n_features, sparsity = s, betawidth = 0)\n",
    "    X, X_test, y, y_test, ss = gen_data(n_samples, n_features, kappa = 100, \n",
    "                                        covariance = sigma, beta = beta)\n",
    "    \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    y -= np.mean(y)\n",
    "    \n",
    "    _, _, coefs  = lars_path(X, y.ravel(), method = 'lasso')\n",
    "\n",
    "    supports = (coefs.T != 0).astype(bool)\n",
    "\n",
    "    # Stick the true model in there\n",
    "    # supports = np.vstack([supports, (beta.ravel() !=0).astype(bool)])\n",
    "    \n",
    "    sa.append(selection_accuracy(beta.ravel(), supports))\n",
    "        \n",
    "    # Fit OLS models to each of the supports\n",
    "    models = []\n",
    "\n",
    "    penalties = np.linspace(0, 2 * np.log(n_samples), 80)\n",
    "    bll_scores_ = np.zeros(supports.shape[0])\n",
    "    MIC_scores_ = np.zeros((supports.shape[0], penalties.size))\n",
    "    BIC_scores_ = np.zeros(supports.shape[0])\n",
    "    mBIC_scores_ = np.zeros(supports.shape[0])\n",
    "    eBIC_scores_ = np.zeros(supports.shape[0])\n",
    "    mBIC2_scores_ = np.zeros(supports.shape[0])\n",
    "    \n",
    "    for j in range(supports.shape[0]):\n",
    "        support = supports[j, :]\n",
    "        if np.count_nonzero(1 * support > 0):\n",
    "            model = LinearRegression().fit(X[:, support] , y)\n",
    "            models.append(model)        \n",
    "            y_pred = model.predict(X[:, support])\n",
    "\n",
    "        else:\n",
    "            models.append(np.nan)\n",
    "            y_pred = np.zeros(y.size)\n",
    "        \n",
    "        bll_scores_[j] = bayesian_lambda_selection(y, y_pred, n_features, np.count_nonzero(1 * support), s, 2)\n",
    "        \n",
    "        MIC_scores_[j, :] =  np.array([MIC(y, y_pred, np.count_nonzero(1 * support), penalty) \n",
    "                                       for penalty in penalties])\n",
    "        \n",
    "        BIC_scores_[j] = BIC(y, y_pred, np.count_nonzero(1 * support))\n",
    "        \n",
    "        mBIC_scores_[j] = mBIC(y, y_pred, np.count_nonzero(1 * support), s)\n",
    "        \n",
    "        eBIC_scores_[j] = eBIC(y, y_pred, X.shape[1], np.count_nonzero(1 * support))\n",
    "        \n",
    "        mBIC2_scores_[j] = mBIC2(y, y_pred, np.count_nonzero(1 * support), s)\n",
    "\n",
    "    \n",
    "    \n",
    "    # For MIC scores, record the oracle selection accuracy and the oracle penalty\n",
    "    selected_models = np.argmin(MIC_scores_, axis = 0)\n",
    "    MIC_selection_accuracies = [selection_accuracy(beta.ravel(), supports[selected_models[j], :]) \n",
    "                                for j in range(selected_models.size)]\n",
    "    oracle_penalty[i] = penalties[np.argmax(MIC_selection_accuracies)]\n",
    "    MIC_oracle_sa[i] = np.max(MIC_selection_accuracies)    \n",
    "\n",
    "    bll_scores.append(bll_scores_)\n",
    "    BIC_scores.append(BIC_scores_)\n",
    "    mBIC_scores.append(mBIC_scores_)\n",
    "    eBIC_scores.append(eBIC_scores_)\n",
    "    mBIC2_scores.append(mBIC2_scores_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle sa\n",
    "plt.plot([np.max(x) for x in sa])\n",
    "\n",
    "plt.plot(MIC_oracle_sa)\n",
    "\n",
    "# BIC sa\n",
    "plt.plot([sa[i][np.argmin(BIC_scores[i])] for i in range(len(sparsity))])\n",
    "# mBIC sa\n",
    "plt.plot([sa[i][np.argmin(mBIC_scores[i])] for i in range(len(sparsity))])\n",
    "# eBIC sa\n",
    "# plt.plot([sa[i][np.argmin(eBIC_scores[i])] for i in range(len(sparsity))])\n",
    "# mBIC2 sa\n",
    "plt.plot([sa[i][np.argmin(mBIC2_scores[i])] for i in range(len(sparsity))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(oracle_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.max(x) for x in sa])\n",
    "\n",
    "plt.plot(MIC_oracle_sa)\n",
    "\n",
    "plt.plot([sa[i][np.argmin(bll_scores[i])] for i in range(len(sparsity))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High SNR with correlations --> might be redeeming for some of the Bayesian approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2919948101043701\n",
      "0.4270005226135254\n",
      "0.7379999160766602\n",
      "2.0329999923706055\n",
      "16.360000133514404\n",
      "0.285999059677124\n",
      "0.41300082206726074\n",
      "0.7270007133483887\n",
      "1.9539985656738281\n",
      "16.16600012779236\n",
      "0.2929995059967041\n",
      "0.4159994125366211\n",
      "0.7300014495849609\n",
      "1.984999656677246\n",
      "16.193000316619873\n",
      "0.3059995174407959\n",
      "0.42599916458129883\n",
      "0.7140014171600342\n",
      "1.9910008907318115\n",
      "16.276998043060303\n",
      "0.30099987983703613\n",
      "0.4120008945465088\n",
      "0.8019993305206299\n",
      "2.042999505996704\n",
      "16.292001962661743\n",
      "0.2909984588623047\n",
      "0.4230005741119385\n",
      "0.7310004234313965\n",
      "1.9999990463256836\n",
      "16.219001531600952\n",
      "0.29600024223327637\n",
      "0.4159996509552002\n",
      "0.7360007762908936\n",
      "1.9969992637634277\n",
      "16.215998888015747\n",
      "0.29400014877319336\n",
      "0.42099833488464355\n",
      "0.7090013027191162\n",
      "2.0029993057250977\n",
      "16.2860004901886\n",
      "0.29600048065185547\n",
      "0.4139993190765381\n",
      "0.7149999141693115\n",
      "1.9690020084381104\n",
      "16.246999740600586\n",
      "0.288999080657959\n",
      "0.41699862480163574\n",
      "0.7279999256134033\n",
      "2.0570015907287598\n",
      "16.304999828338623\n",
      "0.2949988842010498\n",
      "0.4160006046295166\n",
      "0.7180006504058838\n",
      "1.9850001335144043\n",
      "16.392999410629272\n"
     ]
    }
   ],
   "source": [
    "# Just look at the different terms in the Bayes Factor and how they scale with (1) sparsity (2) model penalty (3) number of samples\n",
    "n_features = 50\n",
    "n_samples = np.array([200, 400, 800, 1600, 5000])\n",
    "\n",
    "# Iterate over sparsity. Yield a sequences of models using LARS. Choose a model using the various extensions \n",
    "# of the BIC. \n",
    "\n",
    "sparsity = np.linspace(0.05, 1, 11)\n",
    "sigma = gen_covariance(n_features, 0, n_features, 5, 0)\n",
    "\n",
    "oracle_penalty = np.zeros((sparsity.size, n_samples.size))\n",
    "MIC_oracle_sa = np.zeros((sparsity.size, n_samples.size))\n",
    "\n",
    "# Oracle sa\n",
    "sa = np.empty((sparsity.size, n_samples.size), dtype =  np.dtype('O'))\n",
    "\n",
    "np1 = 80\n",
    "np2 = 120\n",
    "\n",
    "# All the different terms:\n",
    "ll = np.empty((sparsity.size, n_samples.size, np2), dtype =  np.dtype('O'))\n",
    "p1 = np.empty((sparsity.size, n_samples.size, np2), dtype =  np.dtype('O'))\n",
    "BIC = np.empty((sparsity.size, n_samples.size, np2), dtype =  np.dtype('O'))\n",
    "RSS = np.empty((sparsity.size, n_samples.size, np2), dtype =  np.dtype('O'))\n",
    "BIC2 = np.empty((sparsity.size, n_samples.size, np2), dtype =  np.dtype('O'))\n",
    "BIC3 = np.empty((sparsity.size, n_samples.size, np2), dtype =  np.dtype('O'))\n",
    "M_k = np.empty((sparsity.size, n_samples.size, np2), dtype =  np.dtype('O'))\n",
    "P_M = np.empty((sparsity.size, n_samples.size, np2), dtype =  np.dtype('O'))\n",
    "\n",
    "for i, s in enumerate(sparsity):\n",
    "\n",
    "    for ii, ns in enumerate(n_samples):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        penalties = np.linspace(0, 2 * np.log(ns), np1)\n",
    "\n",
    "        bpenalties = np.linspace(-np.log(ns), 2 * np.log(ns), np2)\n",
    "\n",
    "        \n",
    "        beta = gen_beta2(n_features, n_features, sparsity = s, betawidth = 0)\n",
    "        X, X_test, y, y_test, ss = gen_data(ns, n_features, kappa = 5, \n",
    "                                            covariance = sigma, beta = beta)\n",
    "\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        y -= np.mean(y)\n",
    "\n",
    "        _, _, coefs  = lars_path(X, y.ravel(), method = 'lasso')\n",
    "\n",
    "        supports = (coefs.T != 0).astype(bool)\n",
    "\n",
    "        # Stick the true model in there\n",
    "        # supports = np.vstack([supports, (beta.ravel() !=0).astype(bool)])\n",
    "\n",
    "        sa[i, ii] = selection_accuracy(beta.ravel(), supports)\n",
    "\n",
    "        # Fit OLS models to each of the supports\n",
    "        models = []\n",
    "        \n",
    "        MIC_scores_ = np.zeros((supports.shape[0], penalties.size))\n",
    "        \n",
    "        for j in range(supports.shape[0]):\n",
    "            support = supports[j, :]\n",
    "            if np.count_nonzero(1 * support > 0):\n",
    "                model = LinearRegression().fit(X[:, support] , y)\n",
    "                models.append(model)        \n",
    "                y_pred = model.predict(X[:, support])\n",
    "\n",
    "            else:\n",
    "                models.append(np.nan)\n",
    "                y_pred = np.zeros(y.size)\n",
    "\n",
    "            MIC_scores_[j, :] =  np.array([MIC(y, y_pred, np.count_nonzero(1 * support), penalty) \n",
    "                                           for penalty in penalties])\n",
    "\n",
    "            for i3, penalty in enumerate(bpenalties):\n",
    "                # Record all Bayesian terms for each penalty\n",
    "                ll_, p1_, BIC_, BIC2_, BIC3_, M_k_, P_M_ = bayesian_lambda_selection(\n",
    "                                                           y, y_pred, n_features, np.count_nonzero(1 * support),\n",
    "                                                           s, penalty)\n",
    "                ll[i, ii, i3] = ll_\n",
    "                p1[i, ii, i3] = p1_\n",
    "                BIC[i, ii, i3] = BIC_\n",
    "                BIC2[i, ii, i3] = BIC2_\n",
    "                BIC3[i, ii, i3] = BIC3_\n",
    "                M_k[i, ii, i3] = M_k_\n",
    "                P_M[i, ii, i3] = P_M_\n",
    "                \n",
    "        # For MIC scores, record the oracle selection accuracy and the oracle penalty\n",
    "        selected_models = np.argmin(MIC_scores_, axis = 0)\n",
    "        MIC_selection_accuracies = [selection_accuracy(beta.ravel(), supports[selected_models[j], :]) \n",
    "                                    for j in range(selected_models.size)]\n",
    "        oracle_penalty[i, ii] = penalties[np.argmax(MIC_selection_accuracies)]\n",
    "        MIC_oracle_sa[i, ii] = np.max(MIC_selection_accuracies)    \n",
    "\n",
    "        print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given number of samples, track the various terms as vs. the sparsity.\n",
    "\n",
    "# First: Terms that do not involve the penalty:\n",
    "fig, ax = plt.subplots((2, 3), figsize = (15, 15))\n",
    "\n",
    "ax[0].plot(ll)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
